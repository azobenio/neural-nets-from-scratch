{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "import math\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data\n",
    "Same as previous notebook, condensed with less prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, reshape, put to float32\n",
    "trainset = MNIST('../', download=True, train=True)\n",
    "testset = MNIST('../', download=True, train=False)\n",
    "train = trainset.data.reshape(60000, -1).to(torch.float32)\n",
    "test = testset.data.reshape(10000, -1).to(torch.float32)\n",
    "\n",
    "# Normalize\n",
    "m, std = train.mean(), train.std()\n",
    "train = (train - m) / std\n",
    "test = (test - m) / std\n",
    "\n",
    "# Get labels\n",
    "y_train = trainset.targets\n",
    "y_test = testset.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_out, depth):\n",
    "        super().__init__()\n",
    "        # quick 'n dirty way to create wider models\n",
    "        IN = np.linspace(n_in, n_out, depth).astype(int)\n",
    "        self.layers = []\n",
    "        for layer in range(depth-2):\n",
    "            self.layers += [nn.Linear(IN[layer], IN[layer+1])]\n",
    "            self.layers += [nn.ReLU()]\n",
    "        # last layer doesn't have relu\n",
    "        self.layers += [nn.Linear(IN[-2], n_out)]\n",
    "\n",
    "        # cross-entropy is nll(log_softmax), as we defined in previous nb\n",
    "        self.loss = nn.functional.cross_entropy\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([784, 629, 474, 319, 164,  10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_in = train.shape[1]  # 28*28:784\n",
    "n_layers = 6  # now we can just change the number of layers here\n",
    "n_out = 10\n",
    "\n",
    "learner = Model(n_in, n_out, n_layers)\n",
    "np.linspace(n_in, n_out, n_layers).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3047, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.loss(learner(train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing batches\n",
    "so far we've been training the model with the whole data at every epoch. Most of the time this won't be possible (all datasets aren't nearly as small as MNIST). Thus we split the data in small batches (usually 32, or 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0014, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "%flake8_off\n",
    "bs = 64\n",
    "EPOCHS = 5\n",
    "lr = 0.3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range((60000-1)//bs + 1):\n",
    "        # get the batch\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xbatch = train[start_i:end_i]\n",
    "        ybatch = y_train[start_i:end_i]\n",
    "        \n",
    "        # predict and get the loss\n",
    "        output = learner(xbatch)\n",
    "        loss = learner.loss(output, ybatch)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for l in learner.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    # step\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias   -= l.bias.grad   * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias  .grad.zero_()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%flake8_on\n",
    "def accuracy(pred, targs):\n",
    "    return float((pred.argmax(dim=-1) == targs).sum()) / float(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = learner(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9807"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(output, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we have a training loop that works\n",
    "Moreover, last notebook we needed 35 passes through the whole data to get 90% accuracy. With minibatches, the learning is smoother, the model learns more and does so more quickly.\n",
    "\n",
    "Now we'll refactor the code to make it more maintainable (Note that I could've done that from the start, I just wanted to show the potential recruiter that I know what happens behind the scenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## updating the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can define our layers outside the model and pass them as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 5\n",
    "IN = np.linspace(n_in, n_out, depth).astype(int)\n",
    "layers = []\n",
    "for layer in range(depth-2):\n",
    "    layers += [nn.Linear(IN[layer], IN[layer+1])]\n",
    "    layers += [nn.ReLU()]\n",
    "# last layer doesn't have relu\n",
    "layers += [nn.Linear(IN[-2], n_out)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([784, 590, 397, 203,  10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can refactor the Model() class and add each layer as a module\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i, l in enumerate(layers):\n",
    "            self.add_module(f'layer_{i}', l)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Model(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the add_module of the nn.Module class provides us automated access to the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<generator object Module.parameters at 0x7ff0b0754a50>,\n",
       " <bound method Module.parameters of Model(\n",
       "   (layer_0): Linear(in_features=784, out_features=590, bias=True)\n",
       "   (layer_1): ReLU()\n",
       "   (layer_2): Linear(in_features=590, out_features=397, bias=True)\n",
       "   (layer_3): ReLU()\n",
       "   (layer_4): Linear(in_features=397, out_features=203, bias=True)\n",
       "   (layer_5): ReLU()\n",
       "   (layer_6): Linear(in_features=203, out_features=10, bias=True)\n",
       " )>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.parameters(), learner.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4050e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(6.2602e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(8.5853e-05, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# So we can refactor our training loop like so :\n",
    "\n",
    "bs = 64\n",
    "EPOCHS = 4\n",
    "lr = 0.3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range((60000-1)//bs + 1):\n",
    "        # Get the batch\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xbatch = train[start_i:end_i]\n",
    "        ybatch = y_train[start_i:end_i]\n",
    "\n",
    "        # Model\n",
    "        output = learner(xbatch)\n",
    "        loss = nn.functional.cross_entropy(output, ybatch)\n",
    "\n",
    "        # Gradient descent\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in learner.parameters():\n",
    "                p -= p.grad * lr\n",
    "                p.grad.zero_()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So with self.add_module() in the Model Class,\n",
    "# we managed to refactor this part of our training loop:\n",
    "\n",
    "# for l in learner.layers:\n",
    "#     if hasattr(l, 'weight'):\n",
    "#         l.weight -= l.weight.grad * lr\n",
    "#         l.bias   -= l.bias.grad   * lr\n",
    "#         l.weight.grad.zero_()\n",
    "#         l.bias  .grad.zero_()\n",
    "\n",
    "# into:\n",
    "\n",
    "# for p in learner.parameters():\n",
    "#     p -= p.grad * lr\n",
    "#     p.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added code into Model() to use add_module, but in fact, all it does can be done with nn.Sequential\n",
    "(We'll define a func to avoid duplicate code every time we refactor the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear():\n",
    "    learner = nn.Sequential(\n",
    "        nn.Linear(784, 590),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(590, 397),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(397, 203),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(203, 10)\n",
    "    )\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0051, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "learner = get_linear()\n",
    "\n",
    "bs = 64\n",
    "EPOCHS = 4\n",
    "lr = 0.3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range((60000-1)//bs + 1):\n",
    "        # get batch\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xbatch = train[start_i:end_i]\n",
    "        ybatch = y_train[start_i:end_i]\n",
    "\n",
    "        # model\n",
    "        output = learner(xbatch)\n",
    "        loss = nn.functional.cross_entropy(output, ybatch)\n",
    "\n",
    "        # step\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in learner.parameters():\n",
    "                p -= p.grad * lr\n",
    "                p.grad.zero_()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterating through the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is also a bit messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range((60000-1)//bs + 1):\n",
    "#     start_i = i*bs\n",
    "#     end_i = start_i+bs\n",
    "#     xbatch = train[start_i:end_i]\n",
    "#     ybatch = y_train[start_i:end_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = Dataset(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this, we can the the xbatch and ybatch in one line:\n",
    "xbatch, ybatch = my_data[0:64]\n",
    "\n",
    "# But we would still have to calculate the indices each time:\n",
    "# for i in range((n-1)//bs + 1):\n",
    "#    start_i = i*bs\n",
    "#    end_i = start_i+bs\n",
    "\n",
    "# ideally we'd want a one liner like this one:\n",
    "\n",
    "# for xb, yb in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, data, bs): self.data, self.bs = data, bs\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.data), self.bs):\n",
    "            yield self.data[i:i+bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(my_data, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb.shape, yb.shape)\n",
    "    break\n",
    "# if you remove the break and scroll down,\n",
    "# you'll see the last bs isn't 64 but 32.\n",
    "# we'll take a closer look at this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0073, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0018, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0010, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Simplified training loop:\n",
    "learner = get_linear()\n",
    "my_data = Dataset(train, y_train)\n",
    "train_dl = DataLoader(data=my_data, bs=64)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for xb, yb in train_dl:\n",
    "        # Model\n",
    "        output = learner(xb)\n",
    "        loss = nn.functional.cross_entropy(output, yb)\n",
    "\n",
    "        # Gradient descent\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in learner.parameters():\n",
    "                p -= p.grad * lr\n",
    "                p.grad.zero_()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the parameters using a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to simplify the training loop further by getting rid of this loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for p in learner.parameters():\n",
    "#         p -= p.grad * lr\n",
    "#         p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, parameters, lr):\n",
    "        self.parameters, self.lr = list(parameters), lr\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters:\n",
    "                p -= p.grad * self.lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "learner = get_linear()\n",
    "opt = Optimizer(learner.parameters(), 0.4)\n",
    "my_data = Dataset(train, y_train)\n",
    "train_dl = DataLoader(data=my_data, bs=64)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for xb, yb in train_dl:\n",
    "        output = learner(xb)\n",
    "        loss = nn.functional.cross_entropy(output, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have a nice and compact training loop. I said we'd add convolutions but this notebook is getting long so we'll do that in the next notebook, along with \"in-training\" validation to monitor overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
