{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our goal: create a neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "import math\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = MNIST('../', download=True, train=True)\n",
    "testset = MNIST('../', download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 60 000 / 10 000: classic MNIST\n",
    "trainset.data.shape, testset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = trainset.data.reshape(60000, -1)\n",
    "test = testset.data.reshape(10000, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = trainset.targets\n",
    "y_test = testset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FLoating point numbers work better with pytorch...\n",
    "train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.to(torch.float32)\n",
    "test = test.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(33.3184), tensor(78.5675))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data isn't normalized\n",
    "m, std = train.mean(), train.std()\n",
    "m, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3679e-06) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "train = (train - m) / std\n",
    "test = (test - m) / std\n",
    "print(train.mean(), train.std()) # 0 and 1, good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a fully connected neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, neurons=50):\n",
    "    # x is a n by m matrix; to perform matrix operations, we want a 'm by neurons' matrix\n",
    "    m = x.shape[1]\n",
    "    w = torch.randn(m, neurons) * math.sqrt(2/m) # Kaiming He init, works better than Xavier\n",
    "    b = torch.randn(neurons)\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    x = linear(x, 50)\n",
    "    x = relu(x)\n",
    "    x = linear(x, 10)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ten predictions for every row, sounds good\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(output):\n",
    "    return (output.exp() / output.exp().sum(-1, keepdim=True)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-sum-exp-trick:          https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/\n",
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()\n",
    "\n",
    "def log_softmax(x): \n",
    "    return x - x.logsumexp(-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative log likelihood\n",
    "def nll(output, y_train):\n",
    "    return -output[range(y_train.shape[0]), y_train].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9627)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(log_softmax(output), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([60000, 10])\n",
      "loss tensor(3.8240)\n"
     ]
    }
   ],
   "source": [
    "output = model(train)\n",
    "print('output shape:', output.shape)\n",
    "loss = nll(log_softmax(output), y_train)\n",
    "print('loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6006, grad_fn=<NegBackward>)\n",
      "tensor(2.5323, grad_fn=<NegBackward>)\n",
      "tensor(2.1176, grad_fn=<NegBackward>)\n",
      "tensor(2.2939, grad_fn=<NegBackward>)\n",
      "tensor(1.8105, grad_fn=<NegBackward>)\n",
      "tensor(1.6865, grad_fn=<NegBackward>)\n",
      "tensor(1.5013, grad_fn=<NegBackward>)\n",
      "tensor(1.3421, grad_fn=<NegBackward>)\n",
      "tensor(1.1259, grad_fn=<NegBackward>)\n",
      "tensor(0.9166, grad_fn=<NegBackward>)\n",
      "tensor(0.8209, grad_fn=<NegBackward>)\n",
      "tensor(0.8318, grad_fn=<NegBackward>)\n",
      "tensor(0.9769, grad_fn=<NegBackward>)\n",
      "tensor(0.9921, grad_fn=<NegBackward>)\n",
      "tensor(0.9432, grad_fn=<NegBackward>)\n",
      "tensor(0.6969, grad_fn=<NegBackward>)\n",
      "tensor(0.5547, grad_fn=<NegBackward>)\n",
      "tensor(0.4987, grad_fn=<NegBackward>)\n",
      "tensor(0.4755, grad_fn=<NegBackward>)\n",
      "tensor(0.4633, grad_fn=<NegBackward>)\n",
      "tensor(0.4624, grad_fn=<NegBackward>)\n",
      "tensor(0.4721, grad_fn=<NegBackward>)\n",
      "tensor(0.5215, grad_fn=<NegBackward>)\n",
      "tensor(0.5402, grad_fn=<NegBackward>)\n",
      "tensor(0.6184, grad_fn=<NegBackward>)\n",
      "tensor(0.5125, grad_fn=<NegBackward>)\n",
      "tensor(0.4572, grad_fn=<NegBackward>)\n",
      "tensor(0.4154, grad_fn=<NegBackward>)\n",
      "tensor(0.3918, grad_fn=<NegBackward>)\n",
      "tensor(0.3768, grad_fn=<NegBackward>)\n",
      "tensor(0.3672, grad_fn=<NegBackward>)\n",
      "tensor(0.3595, grad_fn=<NegBackward>)\n",
      "tensor(0.3533, grad_fn=<NegBackward>)\n",
      "tensor(0.3477, grad_fn=<NegBackward>)\n",
      "tensor(0.3429, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "# parameters we can change:\n",
    "EPOCHS = 35\n",
    "INPUT = train\n",
    "TARGETS = y_train\n",
    "N_LAYERS = 2\n",
    "LR = 0.3\n",
    "\n",
    "# Automated fully connected neural net creation based on above parameters\n",
    "activations = np.linspace(50, 10, N_LAYERS).astype(int)\n",
    "shapes = [INPUT.shape[1]]\n",
    "shapes.extend(activations)\n",
    "w = []\n",
    "b = []\n",
    "for layer in range(N_LAYERS):\n",
    "    w.append(torch.randn(shapes[layer], activations[layer]) * math.sqrt(2/shapes[layer]))\n",
    "    b.append(torch.zeros(activations[layer]))\n",
    "    w[-1].requires_grad_()\n",
    "    b[-1].requires_grad_()\n",
    "    \n",
    "# Training loop\n",
    "for i in range(EPOCHS):\n",
    "    output = INPUT\n",
    "    for layer in range(N_LAYERS-1):\n",
    "        output = output @ w[layer] + b[layer]\n",
    "        output = relu(output)\n",
    "    \n",
    "    output = output @ w[N_LAYERS-1] + b[N_LAYERS-1]\n",
    "    \n",
    "    loss = nll(log_softmax(output), TARGETS)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for layer in range(N_LAYERS):\n",
    "            w[layer] -= w[layer].grad * LR\n",
    "            b[layer] -= b[layer].grad * LR\n",
    "            w[layer].grad.zero_()\n",
    "            b[layer].grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even with this simple model, we see the loss decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, targs):\n",
    "    return float((pred.argmax(dim=-1) == targs).sum()) / float(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = test\n",
    "for layer in range(N_LAYERS-1):\n",
    "    output = output @ w[layer] + b[layer]\n",
    "    output = relu(output)\n",
    "    \n",
    "output = output @ w[N_LAYERS-1] + b[N_LAYERS-1]\n",
    "output = log_softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9059"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(output, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% accuracy (random would be 10%,as there are 10 classes), our model is definitely learning something.\n",
    "However, this code is messy and hard to maintain. In the next notebook we'll refactor it using python classes. Moreover, fully connected is a good start, but the tool of trade for computer vision is convolutions. We'll add that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
