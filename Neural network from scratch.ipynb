{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our goal: create a neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "import math\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = MNIST('', download=True, train=True)\n",
    "testset = MNIST('', download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 60 000 / 10 000: classic MNIST\n",
    "trainset.data.shape, testset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = trainset.data.reshape(60000, -1)\n",
    "test = testset.data.reshape(10000, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = trainset.targets\n",
    "y_test = testset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FLoating point numbers work better with pytorch...\n",
    "train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.to(torch.float32)\n",
    "test = test.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(33.3184), tensor(78.5675))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data isn't normalized\n",
    "m, std = train.mean(), train.std()\n",
    "m, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (train - m) / std\n",
    "test = (test - m) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a fully connected neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, neurons=50):\n",
    "    w = torch.randn(x.shape[1], neurons) * math.sqrt(2/m)\n",
    "    b = torch.randn(neurons)\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    x = linear(x, 50)\n",
    "    x = relu(x)\n",
    "    x = linear(x, 10)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ten predictions for every row, sounds good\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tens = torch.tensor([3, 2,1, 0.5, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(output):\n",
    "    return (output.exp() / output.exp().sum(-1, keepdim=True)).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax(my_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-sum-exp-trick:          https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/\n",
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()\n",
    "\n",
    "def log_softmax(x): \n",
    "    return x - x.logsumexp(-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(output, y_train):\n",
    "    return -output[range(y_train.shape[0]), y_train].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4309)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(log_softmax(output), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([60000, 10])\n",
      "loss tensor(12.3455)\n"
     ]
    }
   ],
   "source": [
    "output = model(train)\n",
    "print('output shape:', output.shape)\n",
    "loss = nll(log_softmax(output), y_train)\n",
    "print('loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.8638, grad_fn=<NegBackward>)\n",
      "tensor(24.3053, grad_fn=<NegBackward>)\n",
      "tensor(6.5728, grad_fn=<NegBackward>)\n",
      "tensor(2.7249, grad_fn=<NegBackward>)\n",
      "tensor(1.7518, grad_fn=<NegBackward>)\n",
      "tensor(1.5279, grad_fn=<NegBackward>)\n",
      "tensor(1.4192, grad_fn=<NegBackward>)\n",
      "tensor(1.3349, grad_fn=<NegBackward>)\n",
      "tensor(1.2638, grad_fn=<NegBackward>)\n",
      "tensor(1.2018, grad_fn=<NegBackward>)\n",
      "tensor(1.1478, grad_fn=<NegBackward>)\n",
      "tensor(1.1002, grad_fn=<NegBackward>)\n",
      "tensor(1.0581, grad_fn=<NegBackward>)\n",
      "tensor(1.0207, grad_fn=<NegBackward>)\n",
      "tensor(0.9874, grad_fn=<NegBackward>)\n",
      "tensor(0.9573, grad_fn=<NegBackward>)\n",
      "tensor(0.9299, grad_fn=<NegBackward>)\n",
      "tensor(0.9050, grad_fn=<NegBackward>)\n",
      "tensor(0.8820, grad_fn=<NegBackward>)\n",
      "tensor(0.8609, grad_fn=<NegBackward>)\n",
      "tensor(0.8411, grad_fn=<NegBackward>)\n",
      "tensor(0.8228, grad_fn=<NegBackward>)\n",
      "tensor(0.8056, grad_fn=<NegBackward>)\n",
      "tensor(0.7894, grad_fn=<NegBackward>)\n",
      "tensor(0.7742, grad_fn=<NegBackward>)\n",
      "tensor(0.7598, grad_fn=<NegBackward>)\n",
      "tensor(0.7461, grad_fn=<NegBackward>)\n",
      "tensor(0.7332, grad_fn=<NegBackward>)\n",
      "tensor(0.7209, grad_fn=<NegBackward>)\n",
      "tensor(0.7093, grad_fn=<NegBackward>)\n",
      "tensor(0.6981, grad_fn=<NegBackward>)\n",
      "tensor(0.6875, grad_fn=<NegBackward>)\n",
      "tensor(0.6774, grad_fn=<NegBackward>)\n",
      "tensor(0.6676, grad_fn=<NegBackward>)\n",
      "tensor(0.6583, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "# parameters we can change:\n",
    "EPOCHS = 35\n",
    "INPUT = train\n",
    "TARGETS = y_train\n",
    "N_LAYERS = 2\n",
    "LR = 0.3\n",
    "\n",
    "# Automated fully connected neural net creation based on above parameters\n",
    "activations = np.linspace(50, 10, N_LAYERS).astype(int)\n",
    "shapes = [INPUT.shape[1]]\n",
    "shapes.extend(activations)\n",
    "w = []\n",
    "b = []\n",
    "for layer in range(N_LAYERS):\n",
    "    w.append(torch.randn(shapes[layer], activations[layer]) * math.sqrt(2/m))\n",
    "    b.append(torch.zeros(activations[layer]))\n",
    "    w[-1].requires_grad_()\n",
    "    b[-1].requires_grad_()\n",
    "    \n",
    "# Training loop\n",
    "for i in range(EPOCHS):\n",
    "    output = INPUT\n",
    "    for layer in range(N_LAYERS-1):\n",
    "        output = output @ w[layer] + b[layer]\n",
    "        output = relu(output)\n",
    "    \n",
    "    output = output @ w[N_LAYERS-1] + b[N_LAYERS-1]\n",
    "    \n",
    "    loss = nll(log_softmax(output), TARGETS)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for layer in range(N_LAYERS):\n",
    "            w[layer] -= w[layer].grad * LR\n",
    "            b[layer] -= b[layer].grad * LR\n",
    "            w[layer].grad.zero_()\n",
    "            b[layer].grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even with this simple model, we see the loss decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, targs):\n",
    "    return float((pred.argmax(dim=-1) == targs).sum()) / float(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = test\n",
    "for layer in range(N_LAYERS-1):\n",
    "    output = output @ w[layer] + b[layer]\n",
    "    output = relu(output)\n",
    "    \n",
    "output = output @ w[N_LAYERS-1] + b[N_LAYERS-1]\n",
    "output = log_softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(output, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80% accuracy (random would be 10%,as there are 10 classes), our model is definitely learning something.\n",
    "However, this code is messy and hard to maintain. In the next notebook we'll refactor it using python classes. Moreover, fully connected is a good start, but the tool of trade for computer vision is convolutions. We'll add that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
