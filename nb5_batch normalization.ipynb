{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = MNIST('../', download=True, train=True)\n",
    "testset = MNIST('../', download=True, train=False)\n",
    "\n",
    "y_trainset = trainset.targets\n",
    "y_testset = testset.targets\n",
    "\n",
    "# this time, we resize the data to have directly one channel for convolutions\n",
    "trainset = trainset.data.reshape(60000, 1, 28, 28).to(torch.float32)\n",
    "testset = testset.data.reshape(10000, 1, 28, 28).to(torch.float32)\n",
    "\n",
    "# normalize\n",
    "m, s = trainset.mean(), trainset.std()\n",
    "trainset = (trainset - m) / s\n",
    "testset = (testset - m) / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import (not define) useful classes\n",
    "so far we've been defining Dataset, DataLoader and Optimizer. That was mainly an exercise to recreate them from scratch and get a deep understanding of what they do exactly. But we can now import them from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    return (torch.argmax(output, dim=1) == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still, we've got to create our own Dataset Class inheriting from Dataset\n",
    "class MNIST_Dataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = trainset[0:50000, :], trainset[50000:, :]\n",
    "y_train, y_valid = y_trainset[0:50000], y_trainset[50000:]\n",
    "\n",
    "train = MNIST_Dataset(x_train, y_train)\n",
    "valid = MNIST_Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "bs = 64\n",
    "lr = 0.05\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train, bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=4,\n",
    "              kernel_size=3, stride=2, padding=1),  # bs*4*14*14\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(4, 8, 3, 2, 1),  # bs * 8 * 7 * 7\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8, 16, 3, 2, 1),  # bs * 16 * 4 * 4\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, 2, 1),  # bs * 32 * 2 * 2\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, 2, 1),  # bs * 64 * 1 * 1\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(flatten),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "opt = SGD(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.5748570561408997 accuracy: 0.8170780539512634\n",
      "epoch 1 loss: 0.20386554300785065 accuracy: 0.9351114630699158\n",
      "epoch 2 loss: 0.12395601719617844 accuracy: 0.9615843892097473\n",
      "epoch 3 loss: 0.16629882156848907 accuracy: 0.9493431448936462\n",
      "epoch 4 loss: 0.15520162880420685 accuracy: 0.9537221193313599\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        loss = loss_func(model.cuda()(xb.cuda()), yb.cuda())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_acc = 0., 0.\n",
    "        for xb, yb in valid_dl:\n",
    "            pred = model(xb.cuda())\n",
    "            total_loss += loss_func(pred, yb.cuda())\n",
    "            total_acc += accuracy(pred, yb.cuda())\n",
    "        n_entries = len(valid_dl)\n",
    "        print('epoch', epoch,\n",
    "              'loss:', (total_loss/n_entries).item(),\n",
    "              'accuracy:', (total_acc/n_entries).item()\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization \n",
    "<a href='https://arxiv.org/pdf/1502.03167.pdf'> link towards the paper </a><br />\n",
    "The loss varies a lot. This is because we haven't introduced batch normalization into the model. Batch Normalization forces the activations to have unit variance (i.e mean 0 and standard deviation 1). This is important because, since every time we backpropagate the previous layer change, its output distribution will change as well. \n",
    "\n",
    "Quote from the paper:\n",
    "\n",
    "<blockquote>the inputs to each layerare affected by the parameters of all preceding layers – sothat small changes to the network parameters amplify as the network becomes deeper.</blockquote>\n",
    "\n",
    "So the next layer is used to receive numbers with a certain distribution, and suddenly this distribution change! How is this layer supposed to learn which parameters it should have to perform the multiplication with its input, if the input is never similar ?\n",
    "\n",
    "This is why we want the input distribution, at every layer, to be, if not identically, at least similarly distributed.\n",
    "\n",
    "<blockquote>As such it is advantageous for thedistribution of x to remain fixed over time. Then, Θ2 does \n",
    "not have to readjust to compensate for the change in the distribution of x. </blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we do that ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='data/images/BN algorithm.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each minibatch:\n",
    "#     take its mean\n",
    "#     take its variance\n",
    "#     use both to normalize the inputs\n",
    "#     scale and shift using Gamma and Beta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.BatchNorm2d??  # result: source code next cell\n",
    "\n",
    "# Init signature:\n",
    "# nn.BatchNorm2d(\n",
    "#     num_features,\n",
    "#     eps=1e-05,\n",
    "#     momentum=0.1,\n",
    "#     affine=True,\n",
    "#     track_running_stats=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code:\n",
    "\n",
    "```python\n",
    "Source:        \n",
    "class BatchNorm2d(_BatchNorm):\n",
    "    Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\n",
    "    with additional channel dimension) as described in the paper\n",
    "    `Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift`\n",
    "```\n",
    "\\begin{align*}\n",
    "        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
    "\\end{align*}\n",
    "\n",
    "The mean and standard-deviation are calculated per-dimension over\n",
    "the mini-batches and $ `\\gamma` $ and $ `\\beta` $  are learnable parameter vectors\n",
    "of size `C` (where `C` is the input size). By default, the elements of $ `\\gamma` $  are set\n",
    "to 1 and the elements of $ `\\beta` $  are set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, nf, mom=0.1, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.mom, self.eps = mom, eps\n",
    "        # in the paper, gamma and beta are 'parameters to be learned'\n",
    "        self.gamma = nn.Parameter(torch.ones(nf, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(nf, 1, 1))\n",
    "\n",
    "        # use register_buffer when you want a stateful part of your model\n",
    "        # that is not a parameter, but you want it in your state_dict\n",
    "        # Here, we want to keep vars and means for latter inference,\n",
    "        # so we register, but they are not supposed to be affected by\n",
    "        # back prop (they are not parameter: we can't use register_parameter)\n",
    "        self.register_buffer('vars', torch.ones(1, nf, 1, 1))\n",
    "        self.register_buffer('means', torch.zeros(1, nf, 1, 1))\n",
    "\n",
    "    def update_stats(self, x):\n",
    "        # mean((0,2,3)) means \"do a different mean for each channel\"\n",
    "        m = x.mean((0, 2, 3), keepdim=True)\n",
    "        v = x.var((0, 2, 3), keepdim=True)\n",
    "        # we use lerp_ as a proxy for moving average\n",
    "        self.means.lerp_(m, self.mom)\n",
    "        self.vars.lerp_(v, self.mom)\n",
    "        return m, v\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                m, v = self.update_stats(x)\n",
    "        else:\n",
    "            m, v = self.means, self.vars\n",
    "        x = (x-m) / (v+self.eps).sqrt()\n",
    "        return x*self.gamma + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is starting to be messy. We'll see if we can factor this out\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=8,\n",
    "              kernel_size=3, stride=2, padding=1, bias=False),  # bs*4*14*14\n",
    "    BatchNorm(8),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8, 16, 3, 2, 1, bias=False),  # bs * 8 * 7 * 7\n",
    "    BatchNorm(16),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, 2, 1, bias=False),  # bs * 16 * 4 * 4\n",
    "    BatchNorm(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, 2, 1, bias=False),  # bs * 32 * 2 * 2\n",
    "    BatchNorm(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, 2, 1, bias=False),  # bs * 64 * 1 * 1\n",
    "    BatchNorm(64),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(flatten),\n",
    "    nn.Linear(64, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we put Batchnorm after or before relu ?\n",
    "According to the paper, before, but it appears no careful experiment has been done to figure this out:\n",
    "https://forums.fast.ai/t/where-should-i-place-the-batch-normalization-layer-s/56825/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note for future review: at first I had forgotten to set a new optimizer.\n",
    "# As a result, model didn't learn anything.\n",
    "# That's because SGD was then optimizing parameters that didn't exist anymore\n",
    "opt = SGD(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.06602656841278076 accuracy: 0.9791998267173767\n",
      "epoch 1 loss: 0.05816727131605148 accuracy: 0.9821855425834656\n",
      "epoch 2 loss: 0.060302749276161194 accuracy: 0.9821855425834656\n",
      "epoch 3 loss: 0.057873792946338654 accuracy: 0.9835788607597351\n",
      "epoch 4 loss: 0.05947509780526161 accuracy: 0.9819864630699158\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        loss = loss_func(model.cuda()(xb.cuda()), yb.cuda())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_acc = 0., 0.\n",
    "        for xb, yb in valid_dl:\n",
    "            pred = model(xb.cuda())\n",
    "            total_loss += loss_func(pred, yb.cuda())\n",
    "            total_acc += accuracy(pred, yb.cuda())\n",
    "        n_entries = len(valid_dl)\n",
    "        print('epoch', epoch,\n",
    "              'loss:', (total_loss/n_entries).item(),\n",
    "              'accuracy:', (total_acc/n_entries).item()\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the loss is less bumpy. Second: the accuracy jumps to 0.98 ! \n",
    "\n",
    "However, now the model definition is starting to get huge, and each time I want to change the number of filters I have to manually change it through all the layers. That's error-prone and time-consuming. \n",
    "\n",
    "Also, the training loop with validation added is once again messy as well. In the next notebook we'll refactor this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
