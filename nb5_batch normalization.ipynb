{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = MNIST('../', download=True, train=True)\n",
    "testset = MNIST('../', download=True, train=False)\n",
    "\n",
    "y_trainset = trainset.targets\n",
    "y_testset = testset.targets\n",
    "\n",
    "# this time, we resize the data to have directly one channel for convolutions\n",
    "trainset = trainset.data.reshape(60000, 1, 28, 28).to(torch.float32)\n",
    "testset = testset.data.reshape(10000, 1, 28, 28).to(torch.float32)\n",
    "\n",
    "# normalize\n",
    "m, s = trainset.mean(), trainset.std()\n",
    "trainset = (trainset - m) / s\n",
    "testset = (testset - m) / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import (not define) useful classes\n",
    "so far we've been defining Dataset, DataLoader and Optimizer. That was mainly an exercise to recreate them from scratch and get a deep understanding of what they do exactly. But we can now import them from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    return (torch.argmax(output, dim=1) == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still, we've got to create our own Dataset Class inheriting from Dataset\n",
    "class MNIST_Dataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = trainset[0:50000, :], trainset[50000:, :]\n",
    "y_train, y_valid = y_trainset[0:50000], y_trainset[50000:]\n",
    "\n",
    "train = MNIST_Dataset(x_train, y_train)\n",
    "valid = MNIST_Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "bs = 64\n",
    "lr = 0.05\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train, bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=8,\n",
    "              kernel_size=3, stride=2, padding=1),  # bs*8*14*14\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8, 16, 3, 2, 1),  # bs*16*7*7\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, 2, 1),  # bs * 32 * 4 * 4\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, 2, 1),  # bs * 64 * 2 * 2\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, 2, 1),  # bs * 64 * 1 * 1\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(flatten),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "opt = SGD(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1019749641418457\n",
      "0.0021092891693115234\n",
      "0.012062311172485352\n",
      "0.018858611583709717\n",
      "0.09139478206634521\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "for i in range(EPOCHS):\n",
    "    for xb, yb in train_dl:\n",
    "        out = model(xb.cuda())\n",
    "        loss = loss_func(out, yb.cuda())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization \n",
    "The loss varies a lot. This is because we haven't introduced batch normalization into the model. Batch Normalization forces the activations to have unit variance (i.e mean 0 and standard deviation 1) which helps ensure that gradients  . Since the latter layers are high order combinations of the previous layers, batch norm also introduces two parameters that the model can learn to smooth the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
