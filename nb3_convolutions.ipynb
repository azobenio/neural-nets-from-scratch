{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first two notebooks, we have built classes with which we can create a very compact training loop. We kept using fully connected layers for simplicity. We will now add convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, reshape, put to float32\n",
    "trainset = MNIST('../', download=True, train=True)\n",
    "testset = MNIST('../', download=True, train=False)\n",
    "\n",
    "# Get labels\n",
    "y_trainset = trainset.targets\n",
    "y_testset = testset.targets\n",
    "\n",
    "# reshape and pass to float\n",
    "trainset = trainset.data.reshape(60000, -1).to(torch.float32)\n",
    "testset = testset.data.reshape(10000, -1).to(torch.float32)\n",
    "\n",
    "# Normalize\n",
    "m, std = trainset.mean(), trainset.std()\n",
    "trainset = (trainset - m) / std\n",
    "testset = (testset - m) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, data, bs):\n",
    "        self.data, self.bs = data, bs\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.data), self.bs):\n",
    "            yield self.data[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, parameters, lr=0.4):\n",
    "        self.parameters, self.lr = list(parameters), lr\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters:\n",
    "                p -= p.grad * self.lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    return (torch.argmax(output, dim=1) == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "lr = 0.3\n",
    "bs = 64\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "# here we will later download and assign pre-trained models\n",
    "learner = nn.Sequential(\n",
    "    nn.Linear(784, 250),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(250, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "\n",
    "opt = Optimizer(learner.parameters(), lr=lr)\n",
    "\n",
    "my_data = Dataset(trainset, y_trainset)\n",
    "train_dataloader = DataLoader(my_data, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    for xb, yb in train_dataloader:\n",
    "        out = learner(xb)\n",
    "        loss = loss_func(out, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = trainset[0:50000, :], trainset[50000:, :]\n",
    "y_train, y_valid = y_trainset[0:50000], y_trainset[50000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now start using Pytorch's DataLoader because it also has a random sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(Dataset(train, y_train), batch_size=64, shuffle=True)\n",
    "valid_dl = DataLoader(Dataset(valid, y_valid), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.04268951714038849 accuracy: 0.9902468323707581\n",
      "epoch 1 loss: 0.03754099830985069 accuracy: 0.9912420511245728\n",
      "epoch 2 loss: 0.038301967084407806 accuracy: 0.991042971611023\n",
      "epoch 3 loss: 0.033308595418930054 accuracy: 0.9918391704559326\n",
      "epoch 4 loss: 0.03391985222697258 accuracy: 0.9917396306991577\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    learner.train()\n",
    "    for xb, yb in train_dl:\n",
    "        loss = loss_func(learner(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    learner.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_acc = 0., 0.\n",
    "        for xb, yb in valid_dl:\n",
    "            pred = learner(xb)\n",
    "            total_loss += loss_func(pred, yb)\n",
    "            total_acc += accuracy(pred, yb)\n",
    "        n_entries = len(valid_dl)\n",
    "        print('epoch', epoch,\n",
    "              'loss:', (total_loss/n_entries).item(),\n",
    "              'accuracy:', (total_acc/n_entries).item()\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now adding convolutions is just a matter of changing the sequence of layers in nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Jupyter Notebook shortcts to access the doc and see how to use Conv2d\n",
    "# ??nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 9, 28, 28])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can basically replace the nn.Linear with this:\n",
    "my_layer = nn.Conv2d(in_channels=1, out_channels=9, kernel_size=3, padding=1)\n",
    "\n",
    "# Except nn.Conv2d takes input of shape N * Channels * Height * Width \n",
    "# (as seen in the doc if you uncomment above)\n",
    "# and we had no channels so far (MNIST isn't RVB), so we add an extra channel\n",
    "my_layer(xb.reshape(16,1,28,28)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to flatten out the output of the successive convolutions\n",
    "# before we pass it to a nn.Linear()\n",
    "# we add a Lambda layer (pretty much like a lambda function) to do that\n",
    "\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    return x.view(x.shape[0], -1)\n",
    "\n",
    "# we can also use that Lambda class to resize the data as in the above cell\n",
    "def mnist_resize(x): return x.view(-1, 1, 28, 28) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6\n",
    "lr = 0.4\n",
    "bs = 64\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "learner = nn.Sequential(\n",
    "    Lambda(mnist_resize),  # we can do the reshape here\n",
    "    nn.Conv2d(in_channels=1, out_channels=8,\n",
    "              kernel_size=3, stride=1, padding=1),  # bs*8*28*28\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8, 16, 3, 2, 1),  # bs*16*14*14\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, 2, 1),  # bs * 32 * 7 * 7\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, 2, 1),  # bs * 64 * 4 * 4\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, 2, 1),  # bs * 64 * 2 * 2\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(flatten),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "opt = Optimizer(learner.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.06012481078505516 accuracy: 0.9852706789970398\n",
      "epoch 1 loss: 0.09420818835496902 accuracy: 0.9772093892097473\n",
      "epoch 2 loss: 0.33874279260635376 accuracy: 0.918789803981781\n",
      "epoch 3 loss: 0.08275756239891052 accuracy: 0.9787022471427917\n",
      "epoch 4 loss: 0.04851749911904335 accuracy: 0.987957775592804\n",
      "epoch 5 loss: 0.06320108473300934 accuracy: 0.984375\n"
     ]
    }
   ],
   "source": [
    "# A cool thing is that the train loop doesn't need to change\n",
    "for epoch in range(EPOCHS):\n",
    "    learner.train()\n",
    "    for xb, yb in train_dl:\n",
    "        loss = loss_func(learner(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    learner.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_acc = 0., 0.\n",
    "        for xb, yb in valid_dl:\n",
    "            pred = learner(xb)\n",
    "            total_loss += loss_func(pred, yb)\n",
    "            total_acc += accuracy(pred, yb)\n",
    "        n_entries = len(valid_dl)\n",
    "        print('epoch', epoch,\n",
    "              'loss:', (total_loss/n_entries).item(),\n",
    "              'accuracy:', (total_acc/n_entries).item()\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we added convolutions, but two problems arose:\n",
    "\n",
    "* the training isn't very smooth. We'll need to add some regularization\n",
    "* the training is way slower than before. We'll need to start using the GPU\n",
    "\n",
    "We'll start doing that in the next Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
