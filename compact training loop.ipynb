{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "import math\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data\n",
    "Same as previous notebook, condensed with less prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, reshape, put to float32\n",
    "trainset = MNIST('../', download=True, train=True)\n",
    "testset = MNIST('../', download=True, train=False)\n",
    "train = trainset.data.reshape(60000, -1).to(torch.float32)\n",
    "test = testset.data.reshape(10000, -1).to(torch.float32)\n",
    "\n",
    "# Normalize\n",
    "m, std = train.mean(), train.std()\n",
    "train = (train - m) / std\n",
    "test = (test - m) / std\n",
    "\n",
    "# Get labels\n",
    "y_train = trainset.targets\n",
    "y_test = testset.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_out, depth):\n",
    "        super().__init__()\n",
    "        IN = np.linspace(n_in, n_out, depth).astype(int) #quick 'n dirty way to create wider models\n",
    "        self.layers = []\n",
    "        for layer in range(depth-2):\n",
    "            self.layers += [nn.Linear(IN[layer], IN[layer+1])]\n",
    "            self.layers += [nn.ReLU()]\n",
    "        self.layers += [nn.Linear(IN[-2],n_out)] #last layer doesn't have relu\n",
    "        \n",
    "        \n",
    "        self.loss = nn.functional.cross_entropy # cross-entropy is nll(log_softmax), as we defined in previous nb\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: \n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([784, 629, 474, 319, 164,  10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_in = train.shape[1]\n",
    "n_layers = 6 # now we can just change the number of layers here \n",
    "n_out = 10\n",
    "\n",
    "learner = Model(n_in, n_out, n_layers)\n",
    "np.linspace(n_in, n_out, n_layers).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3032, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.loss(learner(train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing batches\n",
    "so far we've been training the model with the whole data at every epoch. Most of the time this won't be possible (all datasets aren't nearly as small as MNIST). Thus we split the data in small batches (usually 32, or 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0021, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0016, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0010, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "EPOCHS = 5\n",
    "lr = 0.3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range((60000-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xbatch = train[start_i:end_i]\n",
    "        ybatch = y_train[start_i:end_i]\n",
    "        output = learner(xbatch) \n",
    "        loss = learner.loss(output, ybatch)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for l in learner.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias   -= l.bias.grad   * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias  .grad.zero_()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, targs):\n",
    "    return float((pred.argmax(dim=-1) == targs).sum()) / float(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = learner(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9764"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(output, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we have a training loop that works\n",
    "Moreover, last notebook we needed 35 passes through the whole data to get 90% accuracy. With minibatches, the learning is smoother, the model learns more and does so more quickly.\n",
    "\n",
    "Now we'll refactor the code to make it more maintainable (Note that I could've done that from the start, I just wanted to show the potential recruiter that I know what happens behind the scenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## updating the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our layers outside the model and pass them as parameters:\n",
    "depth = 5\n",
    "IN = np.linspace(n_in, n_out, depth).astype(int) #quick 'n dirty way to create wider models\n",
    "layers = []\n",
    "for layer in range(depth-2):\n",
    "    layers += [nn.Linear(IN[layer], IN[layer+1])]\n",
    "    layers += [nn.ReLU()]\n",
    "layers += [nn.Linear(IN[-2],n_out)] #last layer doesn't have relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([784, 590, 397, 203,  10])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can refactor the Model() class and add each layer as a module\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i,l in enumerate(layers): self.add_module(f'layer_{i}', l)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<generator object Module.parameters at 0x7f16f1639c00>,\n",
       " <bound method Module.parameters of Model(\n",
       "   (layer_0): Linear(in_features=784, out_features=590, bias=True)\n",
       "   (layer_1): ReLU()\n",
       "   (layer_2): Linear(in_features=590, out_features=397, bias=True)\n",
       "   (layer_3): ReLU()\n",
       "   (layer_4): Linear(in_features=397, out_features=203, bias=True)\n",
       "   (layer_5): ReLU()\n",
       "   (layer_6): Linear(in_features=203, out_features=10, bias=True)\n",
       " )>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the add_module of the nn.Module class provides us automated access to the model parameters\n",
    "learner.parameters(), learner.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0015, grad_fn=<NllLossBackward>)\n",
      "tensor(9.5367e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "tensor(5.8293e-05, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# So we can refactor our training loop like so : \n",
    "\n",
    "bs = 64 \n",
    "EPOCHS = 4 # let's speed up the process a little bit, since we aren't using GPU yet\n",
    "lr = 0.3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range((60000-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xbatch = train[start_i:end_i]\n",
    "        ybatch = y_train[start_i:end_i]\n",
    "        output = learner(xbatch) \n",
    "        loss = nn.functional.cross_entropy(output, ybatch)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in learner.parameters():\n",
    "                p -= p.grad * lr\n",
    "                p.grad.zero_()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So with self.add_module() in the Model Class, \n",
    "# we managed to refactor this part of our training loop:\n",
    "\n",
    "#for l in learner.layers:\n",
    "#    if hasattr(l, 'weight'):\n",
    "#        l.weight -= l.weight.grad * lr\n",
    "#        l.bias   -= l.bias.grad   * lr\n",
    "#        l.weight.grad.zero_()\n",
    "#        l.bias  .grad.zero_()\n",
    "\n",
    "# into:\n",
    "\n",
    "#for p in learner.parameters():\n",
    "#    p -= p.grad * lr\n",
    "#    p.grad.zero_()\n",
    "\n",
    "# We had to add code into Model(), but in fact, all it does can be done with nn.Sequential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll do this to avoid duplicate code every time we refactor the code\n",
    "def get_linear():  \n",
    "    learner = nn.Sequential(\n",
    "    nn.Linear(784, 590),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(590, 397),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(397, 203),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(203, 10)\n",
    "    )\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0269, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "learner = get_linear()\n",
    "\n",
    "bs = 64 \n",
    "EPOCHS = 4 # let's speed up the process a little bit, since we aren't using GPU yet\n",
    "lr = 0.3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range((60000-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xbatch = train[start_i:end_i]\n",
    "        ybatch = y_train[start_i:end_i]\n",
    "        output = learner(xbatch) \n",
    "        loss = nn.functional.cross_entropy(output, ybatch)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in learner.parameters():\n",
    "                p -= p.grad * lr\n",
    "                p.grad.zero_()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterating through the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is also a bit messy:\n",
    "#  \n",
    "#for i in range((60000-1)//bs + 1):\n",
    "#    start_i = i*bs\n",
    "#    end_i = start_i+bs\n",
    "#    xbatch = train[start_i:end_i]\n",
    "#    ybatch = y_train[start_i:end_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = Dataset(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this, we can the the xbatch and ybatch in one line:\n",
    "xbatch, ybatch = my_data[0:64]\n",
    "\n",
    "# But we would still have to calculate the indices each time:\n",
    "# for i in range((n-1)//bs + 1):\n",
    "#    start_i = i*bs\n",
    "#    end_i = start_i+bs\n",
    "\n",
    "# ideally we'd want a one liner like this one:\n",
    "\n",
    "# for xb, yb in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, data, bs): self.data, self.bs = data, bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.data), self.bs): yield self.data[i:i+bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(my_data, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb.shape, yb.shape)\n",
    "    break \n",
    "#if you remove the break and scroll down you'll see the last bs is 32. \n",
    "#we'll take a closer look at this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0072, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0026, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Simplified training loop:\n",
    "learner = get_linear()\n",
    "my_data = Dataset(train, y_train)\n",
    "train_dl = DataLoader(data=my_data, bs=64)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for xb, yb in train_dl:\n",
    "        output = learner(xb) \n",
    "        loss = nn.functional.cross_entropy(output, yb)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for p in learner.parameters():\n",
    "                p -= p.grad * lr\n",
    "                p.grad.zero_()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the parameters using a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to simplify the training loop further by getting rid of this loop:\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for p in learner.parameters():\n",
    "#         p -= p.grad * lr\n",
    "#         p.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, parameters, lr): self.parameters, self.lr = list(parameters), lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters: p -= p.grad * self.lr\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters: p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0063, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0051, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "learner = get_linear()\n",
    "opt = Optimizer(learner.parameters(), 0.4)\n",
    "my_data = Dataset(train, y_train)\n",
    "train_dl = DataLoader(data=my_data, bs=64)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for xb, yb in train_dl:\n",
    "        output = learner(xb) \n",
    "        loss = nn.functional.cross_entropy(output, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have a nice and compact training loop. I said we'd add convolutions but this notebook is getting long so we'll do that in the next notebook, along with \"in-training\" validation to monitor overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
