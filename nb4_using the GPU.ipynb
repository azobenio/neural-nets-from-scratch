{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = MNIST('../', download=True, train=True)\n",
    "testset = MNIST('../', download=True, train=False)\n",
    "\n",
    "y_trainset = trainset.targets\n",
    "y_testset = testset.targets\n",
    "\n",
    "# this time, we resize the data to have directly one channel for convolutions\n",
    "trainset = trainset.data.reshape(60000, 1, 28, 28).to(torch.float32)\n",
    "testset = testset.data.reshape(10000, 1, 28, 28).to(torch.float32)\n",
    "\n",
    "# normalize\n",
    "m, s = trainset.mean(), trainset.std()\n",
    "trainset = (trainset - m) / s\n",
    "testset = (testset - m) / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import (not define) useful classes\n",
    "so far we've been defining Dataset, DataLoader and Optimizer. That was mainly an exercise to recreate them from scratch and get a deep understanding of what they do exactly. But we can now import them from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    return (torch.argmax(output, dim=1) == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still, we've got to create our own Dataset Class inheriting from Dataset\n",
    "class MNIST_Dataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = trainset[0:50000, :], trainset[50000:, :]\n",
    "y_train, y_valid = y_trainset[0:50000], y_trainset[50000:]\n",
    "\n",
    "train = MNIST_Dataset(x_train, y_train)\n",
    "valid = MNIST_Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "bs = 64\n",
    "lr = 0.05\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train, bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=8,\n",
    "              kernel_size=3, stride=2, padding=1),  # bs*8*14*14\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8, 16, 3, 2, 1),  # bs*16*7*7\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, 2, 1),  # bs * 32 * 4 * 4\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, 2, 1),  # bs * 64 * 2 * 2\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, 2, 1),  # bs * 64 * 1 * 1\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(flatten),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "opt = SGD(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5283, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3140, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "CPU times: user 2min 21s, sys: 1min 44s, total: 4min 5s\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(EPOCHS):\n",
    "    for xb, yb in train_dl:\n",
    "        out = model(xb)\n",
    "        loss = loss_func(out, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the GPU to speed up computations requires... well, a gpu. Let's make sure we have one at our disposal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got one. Now, we need to put both the model AND the data on the gpu. Putting a tensor on the gpu in pytorch is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (5): ReLU()\n",
       "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (7): ReLU()\n",
       "  (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (9): AdaptiveAvgPool2d(output_size=1)\n",
       "  (10): Lambda()\n",
       "  (11): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've put the model on the gpu, but not the data. Let's see what happens if we try to model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(xb.shape)\n",
    "# model(xb)  # Uncomment this and you'll see the following error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an input type which is a classic tensor, and a weight type that is a torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0309, -0.0093, -0.0053,  0.0657, -0.0169, -0.0834, -0.0505,  0.1114,\n",
       "          0.0616, -0.0684],\n",
       "        [ 0.0314, -0.0130, -0.0088,  0.0577, -0.0210, -0.0833, -0.0528,  0.1109,\n",
       "          0.0612, -0.0696],\n",
       "        [ 0.0267, -0.0205, -0.0078,  0.0621, -0.0265, -0.0828, -0.0480,  0.1115,\n",
       "          0.0635, -0.0695],\n",
       "        [ 0.0279, -0.0074, -0.0069,  0.0646, -0.0171, -0.0816, -0.0511,  0.1132,\n",
       "          0.0606, -0.0682],\n",
       "        [ 0.0358, -0.0189, -0.0051,  0.0584, -0.0230, -0.0842, -0.0529,  0.1153,\n",
       "          0.0612, -0.0711],\n",
       "        [ 0.0293, -0.0105, -0.0056,  0.0629, -0.0160, -0.0823, -0.0498,  0.1131,\n",
       "          0.0609, -0.0691],\n",
       "        [ 0.0329, -0.0117, -0.0089,  0.0590, -0.0173, -0.0825, -0.0504,  0.1100,\n",
       "          0.0618, -0.0706],\n",
       "        [ 0.0264, -0.0122, -0.0070,  0.0658, -0.0160, -0.0809, -0.0512,  0.1173,\n",
       "          0.0580, -0.0669],\n",
       "        [ 0.0300, -0.0105, -0.0053,  0.0616, -0.0240, -0.0829, -0.0535,  0.1086,\n",
       "          0.0652, -0.0690],\n",
       "        [ 0.0336, -0.0198, -0.0039,  0.0636, -0.0246, -0.0842, -0.0531,  0.1092,\n",
       "          0.0648, -0.0701],\n",
       "        [ 0.0365, -0.0167, -0.0056,  0.0639, -0.0220, -0.0889, -0.0475,  0.1125,\n",
       "          0.0605, -0.0638],\n",
       "        [ 0.0334, -0.0102, -0.0097,  0.0616, -0.0256, -0.0831, -0.0574,  0.1066,\n",
       "          0.0607, -0.0680],\n",
       "        [ 0.0312, -0.0100, -0.0073,  0.0638, -0.0227, -0.0805, -0.0550,  0.1161,\n",
       "          0.0624, -0.0746],\n",
       "        [ 0.0345, -0.0146, -0.0076,  0.0627, -0.0205, -0.0787, -0.0519,  0.1079,\n",
       "          0.0616, -0.0745],\n",
       "        [ 0.0351, -0.0155, -0.0025,  0.0649, -0.0302, -0.0867, -0.0597,  0.1133,\n",
       "          0.0574, -0.0666],\n",
       "        [ 0.0278, -0.0131, -0.0094,  0.0635, -0.0249, -0.0865, -0.0510,  0.1101,\n",
       "          0.0610, -0.0689]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xb.cuda())  # This will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025713443756103516\n",
      "0.00010836124420166016\n",
      "0.015048742294311523\n",
      "0.0007213354110717773\n",
      "0.003766000270843506\n",
      "CPU times: user 9.97 s, sys: 2.3 s, total: 12.3 s\n",
      "Wall time: 7.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.cuda()  # We changed this\n",
    "for i in range(EPOCHS):\n",
    "    for xb, yb in train_dl:\n",
    "        out = model(xb.cuda())  # and this\n",
    "        loss = loss_func(out, yb.cuda())  # and this\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down to 7s, vs 22s earlier. That's 3 times faster! \n",
    "Of course, this improvement will depend on a number of factors: the complexity of the model, your gpu, etc. But a three-times improvement with such a simple architecture shows that using a GPU when you have access to one is simply a no-brainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
