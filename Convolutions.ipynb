{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, reshape, put to float32\n",
    "trainset = MNIST('../', download=True, train=True)\n",
    "testset = MNIST('../', download=True, train=False)\n",
    "\n",
    "# Get labels\n",
    "y_trainset = trainset.targets\n",
    "y_testset = testset.targets\n",
    "\n",
    "# reshape and pass to float\n",
    "trainset = trainset.data.reshape(60000, -1).to(torch.float32)\n",
    "testset = testset.data.reshape(10000, -1).to(torch.float32)\n",
    "\n",
    "# Normalize\n",
    "m, std = trainset.mean(), trainset.std()\n",
    "trainset = (trainset - m) / std\n",
    "testset = (testset - m) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, data, bs): self.data, self.bs = data, bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.data), self.bs): yield self.data[i:i+self.bs]\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, parameters, lr=0.4): self.parameters, self.lr = list(parameters), lr\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters: p -= p.grad * self.lr\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters: p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    return (torch.argmax(output, dim=1)==target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "lr = 0.3\n",
    "bs = 63\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "learner = nn.Sequential(\n",
    "    nn.Linear(784, 250),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(250, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "\n",
    "opt = Optimizer(learner.parameters(), lr=lr)\n",
    "\n",
    "my_data = Dataset(trainset, y_trainset)\n",
    "train_dataloader = DataLoader(my_data, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0015, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0039, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    for xb, yb in train_dataloader:\n",
    "        out = learner(xb)\n",
    "        loss = loss_func(out, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = trainset[0:50000, :], trainset[50000:, :]\n",
    "y_train, y_valid = y_trainset[0:50000], y_trainset[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll now start using Pytorch's DataLoader because it also has a random sampler\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(Dataset(train, y_train), batch_size=64, shuffle=True)\n",
    "valid_dl = DataLoader(Dataset(valid, y_valid), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.043464258313179016 accuracy: 0.9853702187538147\n",
      "epoch 1 loss: 0.05070462077856064 accuracy: 0.9867635369300842\n",
      "epoch 2 loss: 0.03840097039937973 accuracy: 0.9890525341033936\n",
      "epoch 3 loss: 0.030843527987599373 accuracy: 0.9908439517021179\n",
      "epoch 4 loss: 0.037859462201595306 accuracy: 0.9898487329483032\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    learner.train()\n",
    "    for xb, yb in train_dl:\n",
    "        loss = loss_func(learner(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    learner.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss,total_acc = 0.,0.\n",
    "        for xb, yb in valid_dl:\n",
    "            pred = learner(xb)\n",
    "            total_loss += loss_func(pred, yb)\n",
    "            total_acc += accuracy(pred, yb)\n",
    "        n_entries = len(valid_dl)\n",
    "        print('epoch', epoch, \n",
    "              'loss:', (total_loss/n_entries).item(), \n",
    "              'accuracy:', (total_acc/n_entries).item()\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now adding convolutions is just a matter of changing the sequence of layers in nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Jupyter Notebook shortcts to access the doc and see how to use Conv2d\n",
    "#??nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 9, 28, 28])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we can basically replace the nn.Linear with this:\n",
    "my_layer = nn.Conv2d(in_channels=1, out_channels=9, kernel_size=3, padding=1)\n",
    "\n",
    "# Except nn.Conv2d takes input of shape N * Channels * Height * Width \n",
    "# (as seen in the doc if you uncomment above)\n",
    "# and we had no channels so far (MNIST isn't RVB), so we add an extra channel\n",
    "my_layer(xb.reshape(16,1,28,28)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to flatten out the output of the successive convolutions  \n",
    "# before we pass it to a nn.Linear()\n",
    "# we add a Lambda layer (pretty much like a lambda function) to do that\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x): return self.func(x)\n",
    "\n",
    "def flatten(x):      return x.view(x.shape[0], -1)\n",
    "\n",
    "# we can also use that Lambda class to resize the data as in the above cell\n",
    "def mnist_resize(x): return x.view(-1, 1, 28, 28) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6\n",
    "lr = 0.4\n",
    "bs = 64\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "learner = nn.Sequential(\n",
    "    Lambda(mnist_resize), #we can do the reshape here\n",
    "    nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1), # bs*8*28*28\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8, 16, 3, 2, 1), # bs*16*14*14\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, 2, 1), # bs * 32 * 7 * 7\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, 2, 1), # bs * 32 * 4 * 4   \n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, 2, 1), # bs * 32 * 2 * 2 \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(flatten),\n",
    "    nn.Linear(64, 10)  \n",
    ")\n",
    "\n",
    "opt = Optimizer(learner.parameters(), lr=lr)\n",
    "\n",
    "my_data = Dataset(trainset, y_trainset)\n",
    "train_dataloader = DataLoader(my_data, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.11502379924058914 accuracy: 0.9668591022491455\n",
      "epoch 1 loss: 0.08122546225786209 accuracy: 0.9761146306991577\n",
      "epoch 2 loss: 0.06739368289709091 accuracy: 0.9819864630699158\n",
      "epoch 3 loss: 0.17043812572956085 accuracy: 0.9608877301216125\n",
      "epoch 4 loss: 0.4515897333621979 accuracy: 0.9039610028266907\n",
      "epoch 5 loss: 0.06267399340867996 accuracy: 0.9844745397567749\n"
     ]
    }
   ],
   "source": [
    "# A cool thing is that the train loop doesn't need to change\n",
    "for epoch in range(EPOCHS):\n",
    "    learner.train()\n",
    "    for xb, yb in train_dl:\n",
    "        loss = loss_func(learner(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    learner.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss,total_acc = 0.,0.\n",
    "        for xb, yb in valid_dl:\n",
    "            pred = learner(xb)\n",
    "            total_loss += loss_func(pred, yb)\n",
    "            total_acc += accuracy(pred, yb)\n",
    "        n_entries = len(valid_dl)\n",
    "        print('epoch', epoch, \n",
    "              'loss:', (total_loss/n_entries).item(), \n",
    "              'accuracy:', (total_acc/n_entries).item()\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we added convolutions, but two problems arose:\n",
    "\n",
    "* the training isn't very smooth. We'll need to add some regularization\n",
    "* the training is way slower than before. We'll need to start using the GPU\n",
    "\n",
    "We'll start doing that in the next Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
